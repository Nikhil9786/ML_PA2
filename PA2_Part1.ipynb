{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA2-Part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxrikuegLruC",
        "colab_type": "code",
        "outputId": "c1acf168-ce52-405e-c4e0-3489f56d09d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# read data files \n",
        "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
        "files = os.listdir(cwd)  # Get all the files in that directory\n",
        "\n",
        "\n",
        "g = open('reviews.txt','r') # What we know!\n",
        "reviews_all = list(map(lambda x:x[:-1],g.readlines()))\n",
        "g.close()\n",
        "g = open('labels.txt','r') # What we WANT to know!\n",
        "sentiments_all = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
        "g.close()\n",
        "\n",
        "# load vocabulary\n",
        "g = open('vocab.txt','r')\n",
        "vocab = [s.strip() for s in g.readlines()]\n",
        "\n",
        "# Check out sample reviews\n",
        "print('A {} review:'.format(sentiments_all[0]))\n",
        "print(reviews_all[0])\n",
        "print('\\nA {} review:'.format(sentiments_all[1]))\n",
        "print(reviews_all[1])\n",
        "\n",
        "# split into training and test data\n",
        "reviews_train,reviews_test = reviews_all[0:24000],reviews_all[24000:]\n",
        "sentiments_train,sentiments_test = sentiments_all[0:24000],sentiments_all[24000:]\n",
        "\n",
        "# maintain Counter objects to store positive, negative and total counts for\n",
        "# all the words present in the positive, negative and total reviews.\n",
        "positive_word_count = Counter()\n",
        "negative_word_count = Counter()\n",
        "total_counts = Counter()\n",
        "\n",
        "# and increment the counts in the appropriate counter objects\n",
        "# based on the training data\n",
        "\n",
        "for word, label in zip(reviews_train, sentiments_train):\n",
        "    \n",
        "    word_list = word.split()\n",
        "    \n",
        "    if label==\"POSITIVE\":\n",
        "        positive_word_count.update(word_list)\n",
        "        \n",
        "    if label==\"NEGATIVE\":\n",
        "        negative_word_count.update(word_list)\n",
        "        \n",
        "    total_counts.update(word_list)\n",
        "\n",
        "print(positive_word_count.most_common()[:10],\"\\n\")\n",
        "print(negative_word_count.most_common()[-10:-1],\"\\n\")\n",
        "print(total_counts.most_common()[:10],\"\\n\")\n",
        "\n",
        "# maintain a Counter object to store positive to negative ratios \n",
        "pos_neg_ratios = Counter()\n",
        "\n",
        "# Calculate the ratios of positive and negative uses of the most common words\n",
        "# Consider words to be \"common\" if they've been used at least 100 times\n",
        "for term,cnt in (total_counts.most_common()):\n",
        "    if(cnt > 100):\n",
        "        word = term.split()\n",
        "        if negative_word_count[term]!=0:\n",
        "            ratio = positive_word_count[term]/negative_word_count[term]\n",
        "        else:\n",
        "            ratio = positive_word_count[term]\n",
        "            \n",
        "        pos_neg_ratios.update(word)\n",
        "        pos_neg_ratios[term]=ratio\n",
        "        \n",
        "print(pos_neg_ratios.most_common()[:10],\"\\n\")\n",
        "print(pos_neg_ratios.most_common()[-10:-1],\"\\n\")\n",
        "\n",
        "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
        "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
        "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))\n",
        "\n",
        "# take a log of the ratio\n",
        "for word,ratio in pos_neg_ratios.most_common():\n",
        "    pos_neg_ratios[word] = np.log(ratio)\n",
        "print(pos_neg_ratios.most_common()[:10])\n",
        "\n",
        "# visualize the distribution of the log-ratio scores\n",
        "scores = np.array(list(pos_neg_ratios.values()))\n",
        "vocab_selected = list(pos_neg_ratios.keys())\n",
        "\n",
        "h = plt.hist(scores,bins=20)\n",
        "\n",
        "# Print few words with neutral score\n",
        "for ind in np.where(scores == 0)[0][0:10]:\n",
        "    print(vocab_selected[ind])\n",
        "\n",
        "\n",
        "def nonml_classifier(review,pos_neg_ratios):\n",
        "    '''\n",
        "    Function that determines the sentiment for a given review.\n",
        "    \n",
        "    Inputs:\n",
        "      review - A text containing a movie review\n",
        "      pos_neg_ratios - A Counter object containing frequent words\n",
        "                       and corresponding log positive-negative ratio\n",
        "    Return:\n",
        "      sentiment - 'NEGATIVE' or 'POSITIVE'\n",
        "    '''\n",
        "    \n",
        "    reviews = \"POSITIVE NEGATIVE NEUTRAL\"\n",
        "    \n",
        "    pos_neg_reviews = Counter(reviews.split())\n",
        "    \n",
        "    word_list = review.split(\" \")\n",
        "    \n",
        "    for word in word_list:\n",
        "        if pos_neg_ratios[word] >= 0.5:\n",
        "            pos_neg_reviews.update(['POSITIVE'])\n",
        "        elif pos_neg_ratios[word] < 0.5 and pos_neg_ratios[word] > -0.5:\n",
        "            pos_neg_reviews.update(['NEUTRAL'])\n",
        "        else:\n",
        "            pos_neg_reviews.update(['NEGATIVE'])\n",
        "            \n",
        "    if pos_neg_reviews['POSITIVE'] > pos_neg_reviews['NEGATIVE']:\n",
        "        return 'POSITIVE'\n",
        "    else:\n",
        "        return 'NEGATIVE'\n",
        "\n",
        "predictions_test = []\n",
        "for r in reviews_test:\n",
        "    l = nonml_classifier(r,pos_neg_ratios)\n",
        "    predictions_test.append(l)\n",
        "\n",
        "# calculate accuracy\n",
        "correct = 0\n",
        "for l,p in zip(sentiments_test,predictions_test):\n",
        "    if l == p:\n",
        "        correct = correct + 1\n",
        "print('Accuracy of the model = {}'.format(correct/len(sentiments_test)))\n",
        "\n",
        "\n",
        "def create_input_vector(review,word2index):\n",
        "    '''\n",
        "    Function to count how many times each word is used in the given review,\n",
        "    # and then store those counts at the appropriate indices inside x.\n",
        "    '''\n",
        "    vocab_size = len(word2index)\n",
        "    x = np.zeros((1, vocab_size))\n",
        "    for w in review.split(' '):\n",
        "        if w in word2index.keys():\n",
        "            x[0][word2index[w]] += 1\n",
        "    return x\n",
        "\n",
        "def find_ignore_words(pos_neg_ratios):\n",
        "    ignore_words = []\n",
        "    for word in word_list:\n",
        "        if pos_neg_ratios[word] < 0.5 and pos_neg_ratios[word] > -0.5:\n",
        "            del pos_neg_ratios[word]        \n",
        "    return ignore_words\n",
        "\n",
        "# create a word2index mapping from word to an integer index\n",
        "word2index = {}\n",
        "ignore_words = find_ignore_words(pos_neg_ratios)\n",
        "vocab_selected = list(set(vocab_selected).difference(set(ignore_words)))\n",
        "for i,word in enumerate(vocab_selected):\n",
        "    if word not in ignore_words:\n",
        "        word2index[word] = i\n",
        "vocab_size = len(word2index)\n",
        "\n",
        "# Run the script once to generate the file \n",
        "# delete the exiting 'data1.hdf5' file before running it again to avoid error \n",
        "labels_train = np.zeros((len(sentiments_train), 2), dtype=int)\n",
        "labels_test = np.zeros((len(sentiments_test), 2), dtype=int)\n",
        "\n",
        "import h5py\n",
        "with h5py.File('data1.hdf5', 'w') as hf:\n",
        "    hf.create_dataset('data_train', (labels_train.shape[0], vocab_size), np.int16)\n",
        "    hf.create_dataset('data_test', (labels_test.shape[0], vocab_size), np.int16)\n",
        "  # create training data\n",
        "    for i,(r,l) in enumerate(zip(reviews_train, sentiments_train)):\n",
        "        hf[\"data_train\"][i] = create_input_vector(r,word2index)\n",
        "        # one-hot encoding\n",
        "        if l == 'NEGATIVE':\n",
        "            labels_train[i, 0] = 1\n",
        "        else:\n",
        "            labels_train[i, 1] = 1\n",
        "    # create test data\n",
        "    for i,(r,l) in enumerate(zip(reviews_test, sentiments_test)):\n",
        "        hf[\"data_test\"][i] = create_input_vector(r,word2index)\n",
        "        # one-hot encoding\n",
        "        if l == 'NEGATIVE':\n",
        "            labels_test[i, 0] = 1\n",
        "        else:\n",
        "            labels_test[i, 1] = 1\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "tf.compat.v1.random.set_random_seed(10)\n",
        "\n",
        "\n",
        "# parameters of the network\n",
        "learning_rate = 0.01\n",
        "batch_size = 400\n",
        "num_epochs = 50\n",
        "n_input = vocab_size\n",
        "n_classes = 2\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Define weights and biases in Tensorflow according to the parameters set above\n",
        "n_hidden_1 = 10  # 1st layer number of neurons\n",
        "weights = {\n",
        "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "\t'out1': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "\t'out2': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "#value of hidden layers should be lesser than or equal to n_hidden_1 ,second layer\n",
        "#please uncomment this section for the second layer.\n",
        "n_hidden_2 = 10\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'out1': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'out2': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "#value of hidden layers should be lesser than or equal to n_hidden_2 ,third layer \n",
        "#please uncomment this section for the third layer.\n",
        "\n",
        "# n_hidden_3 = 10\n",
        "# weights = {\n",
        "#     'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "#     'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "#     'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "#     'out1': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
        "# }\n",
        "# biases = {\n",
        "#     'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "#     'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "#     'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "#     'out3': tf.Variable(tf.random_normal([n_classes]))\n",
        "#     }\n",
        "\n",
        "\n",
        "\n",
        "def multilayer_perceptron(x):\n",
        "    # define the layers of a single layer perceptron\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "\n",
        "    #Please comment this line for layer2 and layer3\n",
        "    #out_layer = tf.nn.sigmoid(tf.matmul(layer_1, weights['out1']) + biases['out2'])\n",
        "    \n",
        "    #for layer 1 and layer3 comment the below out_layer  and uncomment layer2\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    out_layer = tf.nn.sigmoid(tf.matmul(layer_2, weights['out1']) + biases['out2'])\n",
        "\n",
        "     #for layer 1 and layer 2 comment the below out_layer and uncomment layer3\n",
        "    # layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    # out_layer = tf.nn.sigmoid(tf.matmul(layer_3, weights['out1']) + biases['out3'])\n",
        "\n",
        "    return out_layer\n",
        "\n",
        "logits = multilayer_perceptron(X)\n",
        "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# for some macosx installations, conflicting copies of mpilib causes trouble with tensorflow.\n",
        "# use the following two lines to resolve that issue\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    start_time = time.time()\n",
        "    sess.run(init)\n",
        "\n",
        "    h = h5py.File('data1.hdf5', 'r')\n",
        "    n1 = h.get('data_train') \n",
        "    n2 = h.get('data_test')\n",
        "\n",
        "    # Training cycle\n",
        "    total_batch_train = int(n1.shape[0] / batch_size)\n",
        "    total_batch_test = int(n2.shape[0] / batch_size)\n",
        "\n",
        "    for iter_num in range(num_epochs):\n",
        "        # variables for train and test accuracies\n",
        "        avg_acc_train = 0.\n",
        "        avg_acc_test = 0.\n",
        "        for i in range(total_batch_train):\n",
        "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
        "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
        "\n",
        "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
        "            _label_train = [np.argmax(i) for i in _logits_train]\n",
        "            _label_train_y = [np.argmax(i) for i in train_y]\n",
        "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
        "            avg_acc_train += _accuracy_train\n",
        "\n",
        "\n",
        "        for j in range(total_batch_test):\n",
        "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
        "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
        "\n",
        "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
        "            _label_test = [np.argmax(i) for i in _logits_test]\n",
        "            _label_test_y = [np.argmax(i) for i in test_y]\n",
        "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
        "            avg_acc_test += _accuracy_test\n",
        "\n",
        "        # print the train and test accuracies   \n",
        "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
        "    duration = time.time() - start_time\n",
        "    print('Time elapsed - {} seconds.'.format(duration))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A POSITIVE review:\n",
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "\n",
            "A NEGATIVE review:\n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
            "[('the', 166905), ('.', 153420), ('and', 86383), ('a', 80407), ('of', 74000), ('to', 64131), ('is', 55174), ('in', 48414), ('br', 47307), ('it', 46040)] \n",
            "\n",
            "[('lanter', 1), ('dazzles', 1), ('operational', 1), ('falken', 1), ('henie', 1), ('britons', 1), ('dogsbody', 1), ('ambrosine', 1), ('phillpotts', 1)] \n",
            "\n",
            "[('the', 324087), ('.', 314699), ('and', 157973), ('a', 156729), ('of', 140404), ('to', 130437), ('is', 103358), ('br', 97896), ('it', 92531), ('in', 90435)] \n",
            "\n",
            "[('edie', 109), ('felix', 39.0), ('victoria', 21.9), ('polanski', 19.8), ('matthau', 18.625), ('mildred', 15.428571428571429), ('gandhi', 14.25), ('flawless', 12.444444444444445), ('superbly', 10.181818181818182), ('perfection', 9.615384615384615)] \n",
            "\n",
            "[('waste', 0.0714828897338403), ('unfunny', 0.07083333333333333), ('horrid', 0.06930693069306931), ('mst', 0.04878048780487805), ('incoherent', 0.046875), ('stinker', 0.041237113402061855), ('unwatchable', 0.039603960396039604), ('seagal', 0.02631578947368421), ('uwe', 0.01)] \n",
            "\n",
            "Pos-to-neg ratio for 'the' = 1.0618582280413789\n",
            "Pos-to-neg ratio for 'amazing' = 4.031496062992126\n",
            "Pos-to-neg ratio for 'terrible' = 0.17256637168141592\n",
            "[('edie', 4.6913478822291435), ('felix', 3.6635616461296463), ('victoria', 3.086486636822455), ('polanski', 2.9856819377004897), ('matthau', 2.924504764265623), ('mildred', 2.7362210780689065), ('gandhi', 2.6567569067146595), ('flawless', 2.521274293958875), ('superbly', 2.320603598496724), ('perfection', 2.2633643798407643)]\n",
            "realize\n",
            "hands\n",
            "extreme\n",
            "beat\n",
            "onto\n",
            "psycho\n",
            "test\n",
            "obsessed\n",
            "choose\n",
            "speech\n",
            "Accuracy of the model = 0.782\n",
            "Train acc: 0.536458, Test_acc: 0.551250\n",
            "Train acc: 0.545750, Test_acc: 0.548750\n",
            "Train acc: 0.578625, Test_acc: 0.553750\n",
            "Train acc: 0.541875, Test_acc: 0.576250\n",
            "Train acc: 0.658750, Test_acc: 0.611250\n",
            "Train acc: 0.667167, Test_acc: 0.688750\n",
            "Train acc: 0.754458, Test_acc: 0.725000\n",
            "Train acc: 0.752375, Test_acc: 0.725000\n",
            "Train acc: 0.797500, Test_acc: 0.760000\n",
            "Train acc: 0.820000, Test_acc: 0.791250\n",
            "Train acc: 0.826417, Test_acc: 0.795000\n",
            "Train acc: 0.830000, Test_acc: 0.795000\n",
            "Train acc: 0.840000, Test_acc: 0.791250\n",
            "Train acc: 0.845833, Test_acc: 0.811250\n",
            "Train acc: 0.843083, Test_acc: 0.808750\n",
            "Train acc: 0.858375, Test_acc: 0.820000\n",
            "Train acc: 0.857542, Test_acc: 0.813750\n",
            "Train acc: 0.863333, Test_acc: 0.817500\n",
            "Train acc: 0.863167, Test_acc: 0.822500\n",
            "Train acc: 0.869375, Test_acc: 0.813750\n",
            "Train acc: 0.872208, Test_acc: 0.818750\n",
            "Train acc: 0.873833, Test_acc: 0.806250\n",
            "Train acc: 0.865917, Test_acc: 0.820000\n",
            "Train acc: 0.874458, Test_acc: 0.818750\n",
            "Train acc: 0.874000, Test_acc: 0.817500\n",
            "Train acc: 0.863583, Test_acc: 0.836250\n",
            "Train acc: 0.870583, Test_acc: 0.828750\n",
            "Train acc: 0.881000, Test_acc: 0.826250\n",
            "Train acc: 0.881083, Test_acc: 0.843750\n",
            "Train acc: 0.881583, Test_acc: 0.823750\n",
            "Train acc: 0.889708, Test_acc: 0.835000\n",
            "Train acc: 0.888125, Test_acc: 0.838750\n",
            "Train acc: 0.885500, Test_acc: 0.838750\n",
            "Train acc: 0.894417, Test_acc: 0.837500\n",
            "Train acc: 0.892250, Test_acc: 0.837500\n",
            "Train acc: 0.875917, Test_acc: 0.833750\n",
            "Train acc: 0.893583, Test_acc: 0.837500\n",
            "Train acc: 0.893708, Test_acc: 0.841250\n",
            "Train acc: 0.896417, Test_acc: 0.837500\n",
            "Train acc: 0.897833, Test_acc: 0.837500\n",
            "Train acc: 0.893500, Test_acc: 0.828750\n",
            "Train acc: 0.896292, Test_acc: 0.840000\n",
            "Train acc: 0.896375, Test_acc: 0.838750\n",
            "Train acc: 0.896625, Test_acc: 0.835000\n",
            "Train acc: 0.886333, Test_acc: 0.806250\n",
            "Train acc: 0.896083, Test_acc: 0.823750\n",
            "Train acc: 0.898667, Test_acc: 0.838750\n",
            "Train acc: 0.903500, Test_acc: 0.832500\n",
            "Train acc: 0.901417, Test_acc: 0.828750\n",
            "Train acc: 0.887917, Test_acc: 0.838750\n",
            "Time elapsed - 57.86194443702698 seconds.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}